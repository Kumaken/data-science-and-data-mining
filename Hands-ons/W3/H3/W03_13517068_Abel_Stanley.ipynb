{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Abel Stanley\n",
    "\n",
    "NIM: 13517068"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 03 Handson - Data Preprocessing #02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a learning model, we need to input numerical data to the model. However, we often\n",
    "get non-numerical data as input, e.g., text data. Thus, to use text as input to the learning\n",
    "model, we need to do pre-processing and convert it to numerical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps below are typical pre-processing steps for text data.\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "2. Normalization\n",
    "\n",
    "3. Cleaning\n",
    "\n",
    "4. Lemmatization/stemming\n",
    "Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens1:\n",
      " ['After', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "\n",
      "\n",
      "tokens2:\n",
      " ['Foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# we have two raw texts here that we want to pre-process\n",
    "text1 = \"After watching two hours non stop, \\\n",
    "he says that the film is really fantastic #brilliant.\"\n",
    "text2 = \"Foods sold there are little bit pricy, \\\n",
    "meanwhile the taste is not delicious #notrecommended. \"\n",
    "\n",
    "tokens1 = word_tokenize(text1)\n",
    "print(\"tokens1:\\n\", tokens1)\n",
    "tokens2 = word_tokenize(text2)\n",
    "print(\"\\n\\ntokens2:\\n\", tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code, we try one of normalization processes: converting to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', ',', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '#', 'brilliant', '.']\n",
      "\n",
      "\n",
      "normalized words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', ',', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '#', 'notrecommended', '.']\n"
     ]
    }
   ],
   "source": [
    "# convert to Lower case\n",
    "normalized_words1 = [w.lower() for w in tokens1]\n",
    "print(\"normalized words1:\\n\", normalized_words1)\n",
    "## Normalization\n",
    "normalized_words2 = [w.lower() for w in tokens2]\n",
    "print(\"\\n\\nnormalized words2:\\n\", normalized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 01: remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punc_removedi:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', '', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', '', 'brilliant', '']\n",
      "\n",
      "\n",
      "punc_removed2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', '', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', '', 'notrecommended', '']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punc_removed1 = [w.translate(table) for w in normalized_words1]\n",
    "print(\"punc_removedi:\\n\", punc_removed1)\n",
    "punc_removed2 = [w.translate(table) for w in normalized_words2]\n",
    "print(\"\\n\\npunc_removed2:\\n\", punc_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 02: remove not alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isalpha_words1:\n",
      " ['after', 'watching', 'two', 'hours', 'non', 'stop', 'he', 'says', 'that', 'the', 'film', 'is', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "isalpha_words2:\n",
      " ['foods', 'sold', 'there', 'are', 'little', 'bit', 'pricy', 'meanwhile', 'the', 'taste', 'is', 'not', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "isalpha_words1 = [word for word in punc_removed1 if word.isalpha() ]\n",
    "print(\"isalpha_words1:\\n\", isalpha_words1)\n",
    "isalpha_words2 = [word for word in punc_removed2 if word.isalpha() ]\n",
    "print(\"\\n\\nisalpha_words2:\\n\", isalpha_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning 03: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopWords removedi:\n",
      " ['watching', 'two', 'hours', 'non', 'stop', 'says', 'film', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "stopWords removed2:\n",
      " ['foods', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english' ))\n",
    "#print(\"stop words:\\n\", stop words, \"\\n\") #print this tf you want\n",
    "#to see stop word exampLes\n",
    "stopWords_removed1 = [w for w in isalpha_words1 if not w in stop_words ]\n",
    "print(\"stopWords removedi:\\n\", stopWords_removed1)\n",
    "stopWords_removed2 = [w for w in isalpha_words2 if not w in stop_words ]\n",
    "print(\"\\n\\nstopWords removed2:\\n\", stopWords_removed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed_word1:\n",
      " ['watch', 'two', 'hour', 'non', 'stop', 'say', 'film', 'realli', 'fantast', 'brilliant']\n",
      "\n",
      "\n",
      "stemmed_word2:\n",
      " ['food', 'sold', 'littl', 'bit', 'prici', 'meanwhil', 'tast', 'delici', 'notrecommend']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_word1 = [ps.stem(w) for w in stopWords_removed1 ]\n",
    "print(\"stemmed_word1:\\n\", stemmed_word1)\n",
    "stemmed_word2 = [ps.stem(w) for w in stopWords_removed2 ]\n",
    "print(\"\\n\\nstemmed_word2:\\n\", stemmed_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“lemmatized_words1:\n",
      " ['watching', 'two', 'hour', 'non', 'stop', 'say', 'film', 'really', 'fantastic', 'brilliant']\n",
      "\n",
      "\n",
      "lemmatized_words2:\n",
      " ['food', 'sold', 'little', 'bit', 'pricy', 'meanwhile', 'taste', 'delicious', 'notrecommended']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words1 = [lemmatizer.lemmatize(w) for w in stopWords_removed1]\n",
    "print(\"“lemmatized_words1:\\n\", lemmatized_words1)\n",
    "lemmatized_words2 = [lemmatizer.lemmatize(w) for w in stopWords_removed2]\n",
    "print(\"\\n\\nlemmatized_words2:\\n\", lemmatized_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Converting Preprocessed Text into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical features of text1:\n",
      " [0.         0.31622777 0.         0.31622777 0.31622777 0.\n",
      " 0.31622777 0.         0.         0.31622777 0.         0.\n",
      " 0.31622777 0.31622777 0.         0.31622777 0.         0.31622777\n",
      " 0.31622777] ; shape: (19,)\n",
      "\n",
      "\n",
      "numerical features of text2:\n",
      " [0.33333333 0.         0.33333333 0.         0.         0.33333333\n",
      " 0.         0.33333333 0.33333333 0.         0.33333333 0.33333333\n",
      " 0.         0.         0.33333333 0.         0.33333333 0.\n",
      " 0.        ] ; shape: (19,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# merge two texts into one List (you may also try to use the stemmed_word)\n",
    "two_preprocessed_txt = [lemmatized_words1, lemmatized_words2 ]\n",
    "# define the tfidf vectorizer\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "                         \n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word', #''\n",
    "    tokenizer=dummy ,\n",
    "    preprocessor=dummy,\n",
    "    token_pattern=None )\n",
    "                         \n",
    "# train / learn from the given data\n",
    "model = tfidf.fit(two_preprocessed_txt)\n",
    "# transform to numerical features using the trained model\n",
    "numerical_features = model.transform(two_preprocessed_txt).toarray()\n",
    "''' \n",
    "    ==> these numerical features can then be used by the model,\n",
    "    e.g., for classification to sentiment class: positive and negative\n",
    "'''\n",
    "                         \n",
    "print(\"numerical features of text1:\\n\", numerical_features[0],\n",
    "\"; shape:\", numerical_features[0].shape)\n",
    "print(\"\\n\\nnumerical features of text2:\\n\", numerical_features[1],\n",
    "\"; shape:\", numerical_features[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Question 01 (Q01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What are tokenization, normalization and cleaning?\n",
    "\n",
    "b. What is/are the difference(s) between stemming and lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What are tokenization, normalization and cleaning?\n",
    "##### Tokenization\n",
    "Tokenization is the process of converting text into tokens before transforming it into vectors. It is also easier to filter out unnecessary tokens. For example, a document into paragraphs or sentences into words.\n",
    "\n",
    "\n",
    "#### Normalization\n",
    "Normalization consists of the translation (mapping) of terms in the scheme or linguistic reductions through stemming, lemmatization and other forms of standardization.\n",
    "\n",
    "Words which look different due to casing or written another way but are the same in meaning need to be process correctly. Normalisation processes ensure that these words are treated equally. For example, changing numbers to their word equivalents or converting the casing of all the text.\n",
    "\n",
    "#### Cleaning\n",
    "The cleaning process consists of getting rid of the less useful parts of text through stop-word removal, dealing with capitalization and characters and other details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is/are the difference(s) between stemming and lemmatization?\n",
    "\n",
    "##### 1. The main difference is the way they work and the result that each of them returns:\n",
    "\n",
    "        Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations. Below we illustrate the method with examples in both English and Spanish.\n",
    "    \n",
    "        Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. Again, you can see how it works with the same example words.\n",
    "    \n",
    "##### 2. A lemma is the base form of all its inflectional forms, whereas a stem isn’t. This is why regular dictionaries are lists of lemmas, not stems. This has two consequences:\n",
    "    \n",
    "        First, the stem can be the same for the inflectional forms of different lemmas. This translates into noise in our search results. In fact, it is very common to find entire forms as instances of several lemmas.\n",
    "\n",
    "        Also, the same lemma can correspond to forms with different stems, and we need to treat them as the same word. For example, in Greek, a typical verb has different stems for perfective forms and for imperfective ones. If we were using stemming algorithms we won't be able to relate them with the same verb, but using lemmatization it is possible to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Question 02 (Q02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain what TF-IDF is!\n",
    "\n",
    "Note: \n",
    "\n",
    "(i) you can insert picture (if you want) in the answer, and then upload all the\n",
    "materials (this ipynb file and the pictures) into one zip file to the course portal, \n",
    "\n",
    "(ii) you can also use mathematical equation here, for exampe: you can write log2(Pi) by using\n",
    "$log_{2}(P_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\n",
    "\n",
    "#### What a “relevant word” means\n",
    "\n",
    "We can come up with a more or less subjective definition driven by our intuition: a word’s relevance is proportional to the amount of information that it gives about its context (a sentence, a document or a full dataset). That is, the most relevant words are those that would help us, as humans, to better understand a whole document without reading it all.\n",
    "\n",
    "As pointed out, relevant words are not necessarily the most frequent words since stopwords like “the”, “of” or “a” tend to occur very often in many documents.\n",
    "\n",
    "There is another caveat: if we want to summarize a document compared to a whole dataset about an specific topic (let’s say, movie reviews), there will be words (other than stopwords, like character or plot), that could occur many times in the document as well as in many other documents. These words are not useful to summarize a document because they convey little discriminating power; they say very little about what the document contains compared to the other documents.\n",
    "\n",
    "\n",
    "#### The Algorithm\n",
    "\n",
    "For a term t in a document d, the weight Wt,d of term t in document d is given by:\n",
    "\n",
    "$W(t,d) = TF(t,d) * log (N/DF(t))$\n",
    "\n",
    "Where:\n",
    "\n",
    "$TF(t,d)$ is the number of occurrences of t in document d.\n",
    "\n",
    "$DF(t)$ is the number of documents containing the term t.\n",
    "\n",
    "$N$ is the total number of documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Question 03 (Q03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are other methods that can be used to convert \"preprocessed text\" to \"numerical\n",
    "features\" other than TF-IDF? From what you mention, what are methods that keep the\n",
    "semantic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One possible alternative: Word Embedding\n",
    "\n",
    "Word Embedding converts a word to an n-dimensional vector. Words which are related such as ‘house’ and ‘home’ map to similar n-dimensional vectors, while dissimilar words such as ‘house’ and ‘airplane’ have dissimilar vectors. In this way the ‘meaning’ of a word can be reflected in its embedding, a model is then able to use this information to learn the relationship between words. The benefit of this method is that a model trained on the word ‘house’ will be able to react to the word ‘home’ even if it had never seen that word in training.\n",
    "\n",
    "#### The difference between Word Embedding and TF-IDF\n",
    "\n",
    "Word Embedding:\n",
    "\n",
    "    1. Multi dimensional vector which attempts to capture a word's relationship to other words\n",
    "    2. Often trained on large external corpus\n",
    "    3. Must be applied to each word individually\n",
    "    4. More memory intensive\n",
    "    5. Ideal for problems involving a single word such as a word translation\n",
    "    \n",
    "TF-IDF:\n",
    "\n",
    "    1. Uses a sparse matrix where each word map to just a single value, captures no meaning\n",
    "    2. Trained without external data\n",
    "    3. Can be applied to each training document at once\n",
    "    4. Less memory intensive\n",
    "    5. Ideal for problems with many words and larger document files\n",
    "    \n",
    "#### Method that keeps the semantic in Word Embedding\n",
    "\n",
    "In Word Embedding method, we are able to keep the semantic of each word by constructing a relationship of one word with the others. In order to train a model to do this, we must be able to pass multiple words simultaneously into our model. The solution is to concatenate each word vector together and pass the combined vector.\n",
    "\n",
    "For example:\n",
    "\n",
    "We concatenate every 20 words together where each word is a 300-dimensional embedding and yield a 6,000-dimensional vector. What to do in the case that not every text is exactly 20 words? For cases that are fewer then 20 words we will pad the end of the vector with zeroes, the model will learn not to assign any meaning to these values. For cases that are longer then 20 words we will resort to keeping only the first 20 words and dropping the rest.\n",
    "\n",
    "The result of this algorithm is a machin readable table with the size of number of documents x length of feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "a65c17a7-e1ca-4afb-b7a7-bcf261eb3d3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
